{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(type(last_hidden_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states = last_hidden_states.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_states[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset xnli (C:\\Users\\Brad\\.cache\\huggingface\\datasets\\xnli\\all_languages\\1.1.0\\818164464f9c9fd15776ca8a00423b074344c3e929d00a2c1a84aa5a50c928bd)\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xnli\", \"all_languages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 5010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnli = dataset[\"train\"][\"premise\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': '- و قد ال كريم المفاهيمية اثنان اساسيين - المنتج والجغرافيا .',\n",
       " 'bg': 'концептуално крем краде има две основни измерения - продукт и география .',\n",
       " 'de': 'Konzeptionell cream abschöpfen hat zwei grundlegende Dimensionen - Produkt und Geographie .',\n",
       " 'el': 'Η εννοιολογικά κρέμα κρέμα έχει δύο βασικές διαστάσεις - προϊόν και γεωγραφία .',\n",
       " 'en': 'Conceptually cream skimming has two basic dimensions - product and geography .',\n",
       " 'es': 'Los robando de crema conceptualmente tienen dos dimensiones básicas : producto y geografía .',\n",
       " 'fr': \"L' écrémage conceptuel de la crème a deux dimensions fondamentales : le produit et la géographie .\",\n",
       " 'hi': 'Conceptually क ् रीम एंजलिस में दो मूल आयाम हैं - उत ् पाद और भूगोल ।',\n",
       " 'ru': 'Концептуально крем крем имеет два основных измерения - продукт и география .',\n",
       " 'sw': 'Sakata cream ya conceptually ina vipimo viwili vya msingi - bidhaa na geography .',\n",
       " 'th': 'ท่า ครีม ยักยอก มี สอง มิติ พื้นฐาน   -   สินค้า และ ภูมิศาสตร์',\n",
       " 'tr': 'Kavramsal krem kaymağını iki temel boyutu vardır - ürün ve coğrafya .',\n",
       " 'ur': 'ارضیات کی ناپیدی اور جغرافیہ نے دو بنیادی فسل تصاویر اور جغرافیہ کا اشتراک کیا ہے ۔',\n",
       " 'vi': 'Conceptually kem skimming có hai kích thước cơ bản - sản phẩm và địa lý .',\n",
       " 'zh': '从 概念 上 看 , 奶油 收入 有 两 个 基本 方面 产品 和 地理 .'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnli[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_models in sentence transformers org\n",
    "miniLM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "distil = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
    "xlm = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 690/690 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 114/114 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 1.58M/1.58M [00:00<00:00, 2.89MB/s]\n",
      "Downloading: 100%|██████████| 2.38k/2.38k [00:00<00:00, 149kB/s]\n",
      "Downloading: 100%|██████████| 610/610 [00:00<00:00, 76.8kB/s]\n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 15.2kB/s]\n",
      "Downloading: 100%|██████████| 341/341 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 539M/539M [03:05<00:00, 2.91MB/s] \n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 6.62kB/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 1.96M/1.96M [00:00<00:00, 2.92MB/s]\n",
      "Downloading: 100%|██████████| 531/531 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 996k/996k [00:00<00:00, 2.83MB/s]\n",
      "Downloading: 100%|██████████| 345/345 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 190/190 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 3.74k/3.74k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 718/718 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 122/122 [00:00<00:00, 15.8kB/s]\n",
      "Downloading: 100%|██████████| 229/229 [00:00<?, ?B/s] \n",
      "Downloading: 100%|██████████| 1.11G/1.11G [06:41<00:00, 2.77MB/s]\n",
      "Downloading: 100%|██████████| 53.0/53.0 [00:00<00:00, 6.77kB/s]\n",
      "Downloading: 100%|██████████| 5.07M/5.07M [00:01<00:00, 2.98MB/s]\n",
      "Downloading: 100%|██████████| 150/150 [00:00<00:00, 72.4kB/s]\n",
      "Downloading: 100%|██████████| 9.10M/9.10M [00:03<00:00, 2.98MB/s]\n",
      "Downloading: 100%|██████████| 550/550 [00:00<00:00, 68.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "miniLM_model = SentenceTransformer(miniLM)\n",
    "distil_model = SentenceTransformer(distil)\n",
    "xlm_model = SentenceTransformer(xlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar - و قد ال كريم المفاهيمية اثنان اساسيين - المنتج والجغرافيا .\n",
      "bg концептуално крем краде има две основни измерения - продукт и география .\n",
      "de Konzeptionell cream abschöpfen hat zwei grundlegende Dimensionen - Produkt und Geographie .\n",
      "el Η εννοιολογικά κρέμα κρέμα έχει δύο βασικές διαστάσεις - προϊόν και γεωγραφία .\n",
      "en Conceptually cream skimming has two basic dimensions - product and geography .\n",
      "es Los robando de crema conceptualmente tienen dos dimensiones básicas : producto y geografía .\n",
      "fr L' écrémage conceptuel de la crème a deux dimensions fondamentales : le produit et la géographie .\n",
      "hi Conceptually क ् रीम एंजलिस में दो मूल आयाम हैं - उत ् पाद और भूगोल ।\n",
      "ru Концептуально крем крем имеет два основных измерения - продукт и география .\n",
      "sw Sakata cream ya conceptually ina vipimo viwili vya msingi - bidhaa na geography .\n",
      "th ท่า ครีม ยักยอก มี สอง มิติ พื้นฐาน   -   สินค้า และ ภูมิศาสตร์\n",
      "tr Kavramsal krem kaymağını iki temel boyutu vardır - ürün ve coğrafya .\n",
      "ur ارضیات کی ناپیدی اور جغرافیہ نے دو بنیادی فسل تصاویر اور جغرافیہ کا اشتراک کیا ہے ۔\n",
      "vi Conceptually kem skimming có hai kích thước cơ bản - sản phẩm và địa lý .\n",
      "zh 从 概念 上 看 , 奶油 收入 有 两 个 基本 方面 产品 和 地理 .\n"
     ]
    }
   ],
   "source": [
    "for i in xnli[0]:\n",
    "    print(i + \" \" + xnli[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the data and get word embeddings\n",
    "embed_list = []\n",
    "\n",
    "for idx, i in enumerate(xnli):\n",
    "    new_entry = {}\n",
    "    for i in xnli[idx]:\n",
    "        lang_id = i\n",
    "        lang_value = xnli[idx][i]\n",
    "\n",
    "        embeddings = miniLM_model.encode(lang_value)\n",
    "\n",
    "        new_entry[lang_id] = embeddings\n",
    "    embed_list.append(new_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wcss(vectors):\n",
    "    vectors = list(embed_list[0].values())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster the sentence blocks using k-means and wss\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "vectors = list(embed_list[0].values())\n",
    "kmeans.fit(vectors)\n",
    "y_kmeans = kmeans.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 2, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "embeddings = distil_model.encode(\"I am a sentence\")\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# OLD MODELS\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# m_bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# from transformers import XLMRobertaTokenizer\n",
    "# from transformers import XLMRobertaModel\n",
    "\n",
    "# x_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "# xlm_r = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "# # distilbert-base-multilingual-cased\n",
    "\n",
    "# d_tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "# distil = BertModel.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faa2fac6ff85f643fad080676a73334f6d2a035cd520b8ffec70f2014eef7619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
